{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Point of this Notebook:\n",
    "You can train from where you left off as long as you do not shutdown your kernel. There is no need to save and load weight here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Imports complete-----\n"
     ]
    }
   ],
   "source": [
    "# Loading Common Packages for Our Team\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import xgboost as xgb   # Fan Fei does not have XGBoost on Windows\n",
    "import pickle as pickle\n",
    "import cv2\n",
    "import datetime as dt\n",
    "print(\"------Imports complete-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Model Specific packages\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# callback for saving models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# os.chdir('C:/deep_learning') # This is where the input dataset is stored\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Paths Built-----\n"
     ]
    }
   ],
   "source": [
    "random_seed = 987654321\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# train_path = \"c:/deep_learning/kaggle/amazon/train/\"\n",
    "# test_path = \"c:/deep_learning/kaggle/amazon/test/\"\n",
    "train_path = \"f:/kaggle/amazon/train-jpg/\"\n",
    "test_path = \"f:/kaggle/amazon/test-jpg/\"\n",
    "# train_path = \"/mnt/f/kaggle/amazon/train-jpg/\"\n",
    "# test_path = \"/mnt/f/kaggle/amazon/test-jpg/\"\n",
    "\n",
    "train = pd.read_csv(\"c:/deep_learning/kaggle/amazon/train_v2.csv\")\n",
    "# train = pd.read_csv(\"c:/deep_learning/kaggle/amazon/train_v2_short.csv\")\n",
    "test =  pd.read_csv(\"c:/deep_learning/kaggle/amazon/sample_submission_v2.csv\")\n",
    "# test =  pd.read_csv(\"c:/deep_learning/kaggle/amazon/sample_submission_v2_short.csv\")\n",
    "\n",
    "#Location for saving serialized objects\n",
    "obj_save_path = \"c:/deep_learning/kaggle/amazon/model/\"\n",
    "subm_output_path = \"c:/deep_learning/kaggle/amazon/submissions/\"\n",
    "print(\"------Paths Built-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keras_image_prep(df, data_path, name, dim, labels):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "\n",
    "    # flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    # labels = list(set(flatten([l.split(' ') for l in df['tags'].values])))\n",
    "\n",
    "    label_map = {l: i for i, l in enumerate(labels)}\n",
    "    # inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "    for image_name, tags in tqdm(df.values, miniters=1000):\n",
    "        img = img_to_array(load_img(data_path + image_name + '.jpg', target_size=[dim,dim]))\n",
    "        targets = np.zeros(17)\n",
    "        for t in tags.split(' '):\n",
    "            targets[label_map[t]] = 1 \n",
    "        x_out.append(img)\n",
    "        y_out.append(targets)\n",
    "    \n",
    "    x_out = np.array(x_out, np.float16) / 255.\n",
    "    y_out = np.array(y_out, np.uint8)\n",
    "\n",
    "    print(x_out.shape)\n",
    "    print(y_out.shape)\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 40479/40479 [13:15<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 32, 32, 3)\n",
      "(40479, 17)\n",
      "Train on 35000 samples, validate on 5479 samples\n",
      "Epoch 1/4\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9040Epoch 00000: val_acc improved from -inf to 0.91868, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "35000/35000 [==============================] - 314s - loss: 0.2525 - acc: 0.9040 - val_loss: 0.1995 - val_acc: 0.9187\n",
      "Epoch 2/4\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9201Epoch 00001: val_acc improved from 0.91868 to 0.92785, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "35000/35000 [==============================] - 328s - loss: 0.2041 - acc: 0.9201 - val_loss: 0.1826 - val_acc: 0.9279\n",
      "Epoch 3/4\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9252Epoch 00002: val_acc improved from 0.92785 to 0.92822, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "35000/35000 [==============================] - 336s - loss: 0.1903 - acc: 0.9252 - val_loss: 0.1766 - val_acc: 0.9282\n",
      "Epoch 4/4\n",
      "34944/35000 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9283Epoch 00003: val_acc improved from 0.92822 to 0.93291, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "35000/35000 [==============================] - 345s - loss: 0.1833 - acc: 0.9283 - val_loss: 0.1677 - val_acc: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24704251128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 32\n",
    "labels = train['tags'].str.get_dummies(sep=' ').columns\n",
    "\n",
    "x_train, y_train = keras_image_prep(train, train_path, 'train', input_dim, labels)\n",
    "\n",
    "# this is to provide a local validation F-score\n",
    "split = 35000\n",
    "# split = 80\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(input_dim, input_dim, 3)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "filepath= obj_save_path + \"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# the explicit split approach was taken so as to allow for a local validation\n",
    "# model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5479, 17)\n",
      "(5479, 17)\n",
      "0.837609804454\n"
     ]
    }
   ],
   "source": [
    "# Local Validation Results\n",
    "from sklearn.metrics import fbeta_score\n",
    "p_valid = model.predict(x_valid, batch_size=128,verbose=2)\n",
    "print(y_valid.shape)\n",
    "print(p_valid.shape)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/61191 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 61191/61191 [22:55<00:00, 44.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 32, 32, 3)\n",
      "(61191, 17)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# purge the x_train and y_train dataset, and read in the test images\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test, y_test = keras_image_prep(test, test_path, 'test', input_dim, labels)\n",
    "\n",
    "# predict using the trained model\n",
    "y_test = model.predict(x_test,batch_size=128,verbose=2)\n",
    "y_c_test = (y_test > .222222)*1.0        # before multiplying by 1.0, this appears as an array of True and False\n",
    "print(y_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Submission, using label outputted from long time ago\n",
    "test_labels = []\n",
    "for row in range(y_c_test.shape[0]):\n",
    "    test_labels.append(' '.join(labels[y_c_test[row,:]==1]))\n",
    "Submission_PDFModel = test.copy()\n",
    "Submission_PDFModel.drop('tags', axis = 1)\n",
    "Submission_PDFModel['tags'] = test_labels\n",
    "Submission_PDFModel.to_csv(str(subm_output_path + 'sub_simple_keras_32x32_rd1.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 40479/40479 [17:09<00:00, 47.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 32, 32, 3)\n",
      "(40479, 17)\n",
      "Train on 40479 samples, validate on 5479 samples\n",
      "Epoch 1/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9306Epoch 00000: val_acc improved from 0.93291 to 0.93315, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 346s - loss: 0.1778 - acc: 0.9306 - val_loss: 0.1671 - val_acc: 0.9331\n",
      "Epoch 2/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9323Epoch 00001: val_acc improved from 0.93315 to 0.93558, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 346s - loss: 0.1737 - acc: 0.9323 - val_loss: 0.1606 - val_acc: 0.9356\n",
      "Epoch 3/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9335Epoch 00002: val_acc improved from 0.93558 to 0.93863, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 344s - loss: 0.1694 - acc: 0.9336 - val_loss: 0.1544 - val_acc: 0.9386\n",
      "Epoch 4/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9348Epoch 00003: val_acc did not improve\n",
      "40479/40479 [==============================] - 622s - loss: 0.1656 - acc: 0.9348 - val_loss: 0.1532 - val_acc: 0.9386\n",
      "0.853080735294\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = keras_image_prep(train, train_path, 'train', input_dim, labels)\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_valid, y_valid))\n",
    "p_valid = model.predict(x_valid, batch_size=128,verbose=2)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict using the trained model\n",
    "y_test = model.predict(x_test,batch_size=128,verbose=2)\n",
    "y_c_test = (y_test > .222222)*1.0        # before multiplying by 1.0, this appears as an array of True and False\n",
    "\n",
    "# Build Submission, using label outputted from long time ago\n",
    "test_labels = []\n",
    "for row in range(y_c_test.shape[0]):\n",
    "    test_labels.append(' '.join(labels[y_c_test[row,:]==1]))\n",
    "Submission_PDFModel = test.copy()\n",
    "Submission_PDFModel.drop('tags', axis = 1)\n",
    "Submission_PDFModel['tags'] = test_labels\n",
    "Submission_PDFModel.to_csv(str(subm_output_path + 'sub_simple_keras_32x32_rd2.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40479 samples, validate on 5479 samples\n",
      "Epoch 1/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9354Epoch 00000: val_acc did not improve\n",
      "40479/40479 [==============================] - 363s - loss: 0.1642 - acc: 0.9354 - val_loss: 0.1608 - val_acc: 0.9350\n",
      "Epoch 2/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9364Epoch 00001: val_acc improved from 0.93863 to 0.94153, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 352s - loss: 0.1615 - acc: 0.9364 - val_loss: 0.1452 - val_acc: 0.9415\n",
      "Epoch 3/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9376Epoch 00002: val_acc improved from 0.94153 to 0.94207, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 347s - loss: 0.1578 - acc: 0.9376 - val_loss: 0.1447 - val_acc: 0.9421\n",
      "Epoch 4/4\n",
      "40448/40479 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9383Epoch 00003: val_acc improved from 0.94207 to 0.94314, saving model to c:/deep_learning/kaggle/amazon/model/weights.best.hdf5\n",
      "40479/40479 [==============================] - 353s - loss: 0.1562 - acc: 0.9383 - val_loss: 0.1413 - val_acc: 0.9431\n",
      "0.863012723817\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=4,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(x_valid, y_valid))\n",
    "p_valid = model.predict(x_valid, batch_size=128,verbose=2)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict using the trained model\n",
    "y_test = model.predict(x_test,batch_size=128,verbose=2)\n",
    "y_c_test = (y_test > .222222)*1.0        # before multiplying by 1.0, this appears as an array of True and False\n",
    "\n",
    "# Build Submission, using label outputted from long time ago\n",
    "test_labels = []\n",
    "for row in range(y_c_test.shape[0]):\n",
    "    test_labels.append(' '.join(labels[y_c_test[row,:]==1]))\n",
    "Submission_PDFModel = test.copy()\n",
    "Submission_PDFModel.drop('tags', axis = 1)\n",
    "Submission_PDFModel['tags'] = test_labels\n",
    "Submission_PDFModel.to_csv(str(subm_output_path + 'sub_simple_keras_32x32_rd3.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying out sampling of DataFrame\n",
    "test_short = test.sample(frac=0.25,replace=False)\n",
    "print(test_short.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(Submission_PDFModel.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
